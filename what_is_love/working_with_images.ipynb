{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchvision.transforms import transforms #библиотека для работы с изображениями\n",
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#open CV\n",
    "##import cv2\n",
    "\n",
    "##image = cv2.imread('pok.png')\n",
    "##image.shape\n",
    "\n",
    "#возвращает np.array\n",
    "#открывает картинку в BGR (обратно RGB)\n",
    "#сначала ширина высота, потом каналы"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([4, 2206, 1938])"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from torchvision.io import read_image\n",
    "#сначала количество каналов, а потом ширина - высота\n",
    "\n",
    "image = read_image('pok.png')\n",
    "image.shape\n",
    "\n",
    "\n",
    "#возвращает tensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([1.7025, 1.0056, 1.2843, 0.8057])"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "f_t = transforms.Lambda(lambda x: x*2) #Объект занесенный в функцию должен быть возведен в квадрат\n",
    "a = torch.rand(4)\n",
    "f_t(a)\n",
    "#transforms.  -- посмотреть какие есть функции. Большинство функций работают с изображениями формата PIL (ToPILImage()) - поэтому надо менять форматы\n",
    "#compose - п"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([3.4049, 2.0112, 2.5686, 1.6114])"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "f_t1 = transforms.Compose([f_t, f_t]) #объединяет функции в одну функцию, включая функции из transform.\n",
    "f_t1(a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Если датасет слишком маленький, можно\n",
    "#Изменять изображения - переворачивать, добавлять блюр, менять цветовую гамму\n",
    "#2 Способ - взять натренированную модель. Взять слои которые натренированны на поиске низкоуровневых паттернов. И переобучаем модель на что-то новое"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import Dataset\n",
    "import pathlib \n",
    "#dataset - откуда забрать 1 объект\n",
    "#dataloader - собирает батчи"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Класс того, как будет выдаваться один объект\n",
    "\n",
    "class LatinDataset(Dataset):\n",
    "\n",
    "    #в ините объявляем путь к датасету\n",
    "    def __init__(self, path_dataset: pathlib.Path):\n",
    "        self.path_dataset = path_dataset\n",
    "        self.data_list = [x for x in self.path_dataset.glob('**/*') if x.is_file()] # **/* - паттерн значащий - любая папка внутри и не важно что после неё\n",
    "        # то есть передаем в класс условие по которому ходить по датасету\n",
    "        # ------------\n",
    "        # Выделяем количество классов:\n",
    "        self.data_class = list(set([x.parent for x in self.data_list])) #Используем множество т.к. там значения уникальны. Т.е. убираем повторяющиеся пути\n",
    "        # Возвращаем в лист, т.к. в сете нет индексов, а они нам нужны\n",
    "        # Для свертки важно лишь количество фильтров, а не размерность картинки.\n",
    "        # Но для батчей все картинки должны быть одной размерности. Поэтому все картинки нужно подогнать к одному размеру.\n",
    "        self.transform_func = transforms.Compose([\n",
    "             transforms.ToPILImage(),\n",
    "             transforms.Grayscale(), #т.к. мы работаем с чб картинками| 1 фильтр\n",
    "             transforms.Resize((124, 124)),\n",
    "             transforms.ToTensor()\n",
    "        ])\n",
    "\n",
    "    def __len__(self): #всегда определять длину. Т.к. обращаемся к датасету он должен возвращать максимальное кол-во объектов в датасете\n",
    "        return len(self.data_list)\n",
    "    \n",
    "    def __getitem__(self, index): #dataloader returns some index from datalist\n",
    "        img_path = self.data_list[index] #path to the image\n",
    "        #but path also returns label\n",
    "        #label вектор размерности максимального размера класса, где единица указывает принадлежность к классу (см. one hot encoder)\n",
    "        img_label = torch.zeros(len(self.data_class)) #возвращает тензор с нулевыми значениями с размерностью кол-ва классов\n",
    "        img_label[self.data_class.index(img_path.parent)] = 1.0 #Находим индекс класса и заменяем ячейку под этим индексом на принадлежность к классу.\n",
    "        #В нашем случае каждая папка это отдельный класс\n",
    "        img_tensor = read_image(str(img_path)) #Нужно str т.к. read_image не работает с типом WindowsPath, только str\n",
    "        return self.transform_func(img_tensor), img_label\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# g = pathlib.Path(r'C:\\Users\\Joe\\Desktop\\python_yes\\what_is_love\\Latin')\n",
    "# a = [x for x in g.glob('**/*') if x.is_file()]\n",
    "# set([x.parent for x in a])\n",
    "# #[a for a in g.iterdir()]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Pytorch это динамически генерируемы граф <br>\n",
    "Поэтому нужно инициализировать все начальные слои, а потом описать взаимодействие функций в классе"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn #Модули для построения нейронных сетей. Обычно торч работает с тензорами и подобным\n",
    "# nn - модель нейросети. Но также и слой (наприм. слой с функцией активации)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CVModel(nn.Module):\n",
    "\n",
    "    def __init__(self):\n",
    "        super(CVModel, self).__init__() # Initialize self._modules as OrderedDict\n",
    "        self.conv_1 = nn.Conv2d(1, 32, (3, 3)) #Объявляем свёрточный слой. Ему необходимо несколько параметров: кол-во слоев, кол-во фильтров, размер фильтра (окна которым ходим по картинке)\n",
    "        # 64 фильтра(паттерна) на цветной картинке стоит\n",
    "        self.conv_2 = nn.Conv2d(32, 64, (3, 3)) #Высокоуровневые паттерны (кол-во фильтров обычно увеличивают в 2-3 раза)\n",
    "        self.pool_1 = nn.MaxPool2d((2, 2)) #Уменьшаем входящую картинку на 2\n",
    "        # Нужно всегда просчитывать размер картинки на фыходе, чтобы не искать свертку 1 пикселя\n",
    "        # Если картинка размера (n, n), окна (m, m) и шаг окна = 1. То на выходе изображение будер размера (n-m+1, n-m+1)\n",
    "        self.conv_3 = nn.Conv2d(64, 128, (3, 3))\n",
    "        self.conv_4 = nn.Conv2d(128, 128, (3, 3))\n",
    "        self.pool_2 = nn. MaxPool2d((2, 2))\n",
    "        self.glob_pool = nn.MaxPool2d((28, 28)) #Растянули матрицу на вектор\n",
    "        self.linear = nn.Linear(128, 26) #26 классов - кол-во букв в алфавите  #Linear + Softmax ~ многоклассовая логистическая классификация\n",
    "        self.softmax = nn.Softmax()\n",
    "# (124, 124, 1) -> (122, 122, 32) -> (120, 120, 64) -> (60, 60, 64) -> (58, 58, 128) -> (56, 56, 128) -> (28, 28, 128) -> (1, 1, 128)\n",
    "#                  32 матрицы размером 122х122          pooling\n",
    "# Linear - создает 26 ячеек в каждой из которых проводится линейная операция на вектором размерностью 128. Инициализируем 26 линейных регрессий. Каждая регрессия определяет принадлежит ли объет к классу или нет (булиевая классификация)\n",
    "\n",
    "    def forward(self, X):\n",
    "        res = self.conv_1(X)\n",
    "        res = self.conv_2(res)\n",
    "        res = self.pool_1(res)\n",
    "        res = self.conv_3(res)\n",
    "        res = self.conv_4(res)\n",
    "        res = self.pool_2(res)\n",
    "        res = self.glob_pool(res) #(1, 1, 128)\n",
    "        res = self.linear(res.view(-1, 128)) #Сокращение размерности до (128)\n",
    "        return self.softmax(res)\n",
    "# loss should be CrossEntropy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "path = pathlib.Path(r'C:\\Users\\Joe\\Desktop\\python_yes\\what_is_love\\Latin')\n",
    "loss = nn.CrossEntropyLoss()\n",
    "dataset = LatinDataset(path)\n",
    "model = CVModel()\n",
    "trainloader = DataLoader(dataset, batch_size=32, shuffle=True)\n",
    "optimizer = torch.optim.RMSprop(params=model.parameters())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_one_epoch(epoch, trainloader, optimizer, model, loss, dataset):\n",
    "    correct = 0\n",
    "    epoch_loss = 0.0 \n",
    "    for i, data in enumerate(trainloader, 0):\n",
    "        x, y = data\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(x)\n",
    "        error = loss(outputs, y)\n",
    "        error.backward()\n",
    "        epoch_loss += outputs.shape[0] * error.item()\n",
    "        optimizer.step()\n",
    "        outputs = (outputs > 0.5).float()\n",
    "        correct += (outputs == y).float().sum()\n",
    "\n",
    "    print(f'TRAIN [{epoch + 1}] loss: {epoch_loss/ dataset.__len__():.3f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Joe\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\torch\\nn\\modules\\module.py:1518: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
      "  return self._call_impl(*args, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TRAIN [1] loss: 3.283\n"
     ]
    }
   ],
   "source": [
    "for epoch in range(1):\n",
    "    train_one_epoch(epoch, trainloader, optimizer, model, loss, dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def train_one_epoch(epoch, trainloader, optimizer, model, loss, dataset):\n",
    "#     #кол-во эпох, оптимизатор - из старых весов отнимаем значения градиента и получаем новое\n",
    "#     correct = 0\n",
    "#     epoch_loss = 0.0 #для себя чтобы следить за изменение й ошибки\n",
    "#     for i, data in enumerate(trainloader, 0):\n",
    "#         x, y = data #так как датасет возвращает 2 значения\n",
    "#         optimizer.zero_grad()\n",
    "#         outputs = model(x) #результаты модели\n",
    "#         loss = loss(outputs, y) #функция ошибки\n",
    "#         loss.backward() #обратные градиенты (т.е считает призводную кажд элемента)\n",
    "#         epoch_loss += outputs.shape[0] * loss.item() #записываем значения ошибок\n",
    "#         optimizer.step() #совершает ход вычитания производных и старых w\n",
    "#         outputs = (outputs > 0.5).float() #для себя\n",
    "#         correct += (outputs == y).float().sum()\n",
    "\n",
    "#     print(f'TRAIN [{epoch + 1}] loss: {epoch_loss/ dataset.__len__():.3f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
