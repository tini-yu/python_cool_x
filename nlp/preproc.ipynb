{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.corpus import stopwords\n",
    "import string\n",
    "from nltk.tokenize import RegexpTokenizer\n",
    "import spacy\n",
    "nlp = spacy.load(\"ru_core_news_sm\")\n",
    "import gensim.downloader\n",
    "\n",
    "import sklearn\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.metrics.pairwise import cosine_similarity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = RegexpTokenizer('\\w+')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Метод обработки текстов:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "def lemmatize(text):\n",
    "    while '»' in text:\n",
    "        text = text.replace('»', '')\n",
    "    while '«' in text:\n",
    "        text = text.replace('«', '')\n",
    "    while '(' in text:\n",
    "        text = text.replace('(', '')\n",
    "    while ')' in text:\n",
    "        text = text.replace(')', '')\n",
    "\n",
    "    symbs = ['—', \"»\", '«', 'note', '…']\n",
    "    stop = set(stopwords.words('russian') + list(string.punctuation) + symbs)\n",
    "\n",
    "    tokens = [i for i in tokenizer.tokenize(text.lower()) if i not in stop]\n",
    "\n",
    "    lemmatized = []\n",
    "    for word in tokens:\n",
    "        doc = nlp(word)\n",
    "        for token in doc:\n",
    "            lemmatized.append(token.lemma_)\n",
    "\n",
    "    processed = ' '.join(lemmatized)\n",
    "\n",
    "    return processed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "texts = ['breter_1846.txt', 'burmistr_1847.txt', 'mumu_1854.txt', 'asya_1858.txt', 'otcy_i_deti_1862.txt', 'korol_lir_1870.txt']\n",
    "\n",
    "all_texts = []\n",
    "for el in texts:\n",
    "    with open(el, 'r') as file:\n",
    "        text = file.read()\n",
    "    all_texts.append(lemmatize(text))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Косинусное сравнение текстов:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    breter   | burmistr |   mumu   |   asya   |otcy&deti | korol lir\n",
      "[[1.         0.29623983 0.37682161 0.47978    0.46451692 0.39761038]\n",
      " [0.29623983 1.         0.31666213 0.38300949 0.48388489 0.37470989]\n",
      " [0.37682161 0.31666213 1.         0.48101979 0.45646494 0.44660928]\n",
      " [0.47978    0.38300949 0.48101979 1.         0.59723527 0.54452831]\n",
      " [0.46451692 0.48388489 0.45646494 0.59723527 1.         0.60077666]\n",
      " [0.39761038 0.37470989 0.44660928 0.54452831 0.60077666 1.        ]]\n"
     ]
    }
   ],
   "source": [
    "count_v = CountVectorizer()\n",
    "sparse_matrix = count_v.fit_transform(all_texts)\n",
    "\n",
    "print('    breter   | burmistr |   mumu   |   asya   |otcy&deti | korol lir')\n",
    "print(cosine_similarity(sparse_matrix))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
